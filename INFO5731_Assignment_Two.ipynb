{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Larai15/kubernetes/blob/master/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#wuhancoronovirus\"](https://twitter.com/hashtag/wuhancoronovirus) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests         \n",
        "from bs4 import BeautifulSoup  \n",
        "import csv\n",
        "\n",
        "headers = {\n",
        "        'Accept-Encoding': 'gzip, deflate, sdch',\n",
        "        'Accept-Language': 'en-US,en;q=0.8',\n",
        "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "        'Referer': 'http://www.wikipedia.org/',\n",
        "        'Connection': 'keep-alive',\n",
        "}\n",
        "\n",
        "#AMAZON\n",
        "def scrap_amazon(scrap_link):\n",
        "  #scrap_link = \"https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1\"\n",
        "  req = requests.get(scrap_link)\n",
        "  soup = BeautifulSoup(req.content, 'html.parser')\n",
        "  container = soup.findAll('div', attrs={'class':'a-section review aok-relative'})\n",
        "  data = []\n",
        "  for x in container:\n",
        "      Reviewer = x.find('a', attrs={'class':'review-title'}).text\n",
        "      ReviewerName = x.find('span', attrs={'class':'a-profile-name'}).text\n",
        "      data.append([ReviewerName,Reviewer.strip(\"\\n\")])\n",
        "  with open('amazon.csv', 'w', newline='') as csvfile:\n",
        "      csvwriter = csv.writer(csvfile)\n",
        "      csvwriter.writerow(['User','Review'])\n",
        "      for review in data:\n",
        "          csvwriter.writerow(review)\n",
        "\n",
        "#JOKER\n",
        "\n",
        "def scrape_joker(url):\n",
        "    response = requests.get(url,headers=headers, timeout=10)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    reviews = []\n",
        "    users  = []\n",
        "    count  = 0\n",
        "    for j in soup.findAll(\"a\",attrs={'class':'title'}):\n",
        "            if count >= 100:\n",
        "                break\n",
        "            reviews.append(j.text)\n",
        "            count += 1\n",
        "    count  = 0\n",
        "    for j in soup.findAll(\"span\",attrs={'class':'display-name-link'}):\n",
        "            if count >= 100:\n",
        "                break\n",
        "            users.append(j.text)\n",
        "            count += 1\n",
        "\n",
        "    with open('imdb.csv', 'w') as csv_file:\n",
        "        csvwriter = csv.writer(csv_file,delimiter=',')\n",
        "        csvwriter.writerow([\"USER\",\"REVIEW\"])\n",
        "        for i in range(len(reviews)):\n",
        "            csvwriter.writerow([users[i],reviews[i]])\n",
        "def scrap_citeseer(scrap_link):\n",
        "  req = requests.get(scrap_link)\n",
        "  soup = BeautifulSoup(req.content, 'html.parser')\n",
        "  container = soup.findAll('a', attrs={'class':'remove doc_details'})\n",
        "\n",
        "\n",
        "  with open('citeseer.csv', 'w', newline='') as csvfile:\n",
        "      csvwriter = csv.writer(csvfile)\n",
        "      csvwriter.writerow(['PAPER'])\n",
        "      for x in container[:100]:\n",
        "          csvwriter.writerow([str(x.text).strip()])\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    url = \"https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1\"\n",
        "    scrap_amazon(url)\n",
        "    url = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
        "    scrape_joker(url)\n",
        "    url = \"https://citeseerx.ist.psu.edu/search?q=natural+language+processing\"\n",
        "    scrap_citeseer(url)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "outputId": "9fa88555-ea95-41b7-f67a-65788ad04249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import csv\n",
        "import pandas as pd \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def clean_text(train):\n",
        "    result = {}\n",
        "    split_train = train.split()\n",
        "\n",
        "    #1.2\n",
        "    train = train.translate(str.maketrans('', '', string.punctuation)) #remove punctuation\n",
        "    train = train.translate(str.maketrans('', '', string.digits)) #remove digits\n",
        "\n",
        "    train = \" \".join(x for x in train.split() if x not in stop) #remove stop words\n",
        "\n",
        "    train = train.lower() #lowercase\n",
        "\n",
        "    st = PorterStemmer()\n",
        "    train = \" \".join(st.stem(x) for x in train.split())\n",
        "\n",
        "    train = \" \".join(Word(x).lemmatize() for x in train.split())\n",
        "    return train\n",
        "#Amazon\n",
        "cleaned = []\n",
        "with open(\"amazon.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[1]\n",
        "    train = clean_text(train)\n",
        "    row.append(train)\n",
        "    cleaned.append(row)\n",
        "\n",
        "with open(\"amazon_clean.csv\",\"w\") as f:\n",
        "    csvwriter = csv.writer(f)\n",
        "    for l in cleaned:\n",
        "       csvwriter.writerow(l)\n",
        "  \n",
        "\n",
        "\n",
        "#IMDB\n",
        "cleaned = []\n",
        "with open(\"imdb.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[1]\n",
        "    train = clean_text(train)\n",
        "    row.append(train)\n",
        "    cleaned.append(row)\n",
        "\n",
        "with open(\"imdb_clean.csv\",\"w\") as f:\n",
        "    csvwriter = csv.writer(f)\n",
        "    for l in cleaned:\n",
        "       csvwriter.writerow(l)\n",
        "\n",
        "#CITESEER\n",
        "cleaned = []\n",
        "with open(\"citeseer.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[0]\n",
        "    train = clean_text(train)\n",
        "    row.append(train)\n",
        "    cleaned.append(row)\n",
        "\n",
        "with open(\"citeseer_clean.csv\",\"w\") as f:\n",
        "    csvwriter = csv.writer(f)\n",
        "    for l in cleaned:\n",
        "       csvwriter.writerow(l)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab_type": "code",
        "outputId": "36255766-db67-403d-bae3-83ab7166d0df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "import csv\n",
        "from pprint import pprint\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def train_pos_tag(train):\n",
        "    tokens = nltk.word_tokenize(train)\n",
        "    pos_tag = nltk.pos_tag(tokens)\n",
        "    return pos_tag\n",
        "def train_chunk(sentence):\n",
        "    result = {}\n",
        "    doc = nlp(sentence)\n",
        "    # doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "    for ent in doc.ents: \n",
        "        if ent.label_ in result.keys():\n",
        "            result[ent.label_] += 1\n",
        "        else:\n",
        "            result[ent.label_] = 1\n",
        "    return result\n",
        "     \n",
        "def train_parts(post_tag):\n",
        "    count = {\"NN\":0,\"VB\":0,\"JJ\":0,\"RB\":0}\n",
        "    for tag in pos_tag:\n",
        "        if tag[1] in count.keys():\n",
        "            count[tag[1]] += 1\n",
        "    return {\"text\":train,\"noun\":count[\"NN\"],\"verb\":count[\"VB\"],\"adjective\":count[\"JJ\"],\"adverb\":count[\"RB\"]}\n",
        "\n",
        "#Amazon\n",
        "print(\"\\nAMAZON\\n\")\n",
        "cleaned = []\n",
        "with open(\"amazon_clean.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[2]\n",
        "    # print(train)\n",
        "    pos_tag = train_pos_tag(train)\n",
        "    parts = train_parts(pos_tag)\n",
        "    print(parts)\n",
        "    chunk = train_chunk(row[1])\n",
        "    print(chunk)\n",
        "\n",
        "#IMDB\n",
        "print(\"\\imdb\\n\")\n",
        "cleaned = []\n",
        "with open(\"imdb_clean.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[2]\n",
        "    pos_tag = train_pos_tag(train)\n",
        "    parts = train_parts(pos_tag)\n",
        "    print(parts)\n",
        "    chunk = train_chunk(row[1])\n",
        "    print(chunk)\n",
        "\n",
        "\n",
        "\n",
        "#CITESEER\n",
        "print(\"\\nCITESEER\\n\")\n",
        "cleaned = []\n",
        "with open(\"citeseer_clean.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[1]\n",
        "    pos_tag = train_pos_tag(train)\n",
        "    parts = train_parts(pos_tag)\n",
        "    print(parts)\n",
        "    chunk = train_chunk(row[1])\n",
        "    print(chunk)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "\n",
            "AMAZON\n",
            "\n",
            "\\imdb\n",
            "\n",
            "{'text': 'a viewer actual went tiff wit film didnt want believ hype absolut masterpiec phoenix certifi legend', 'noun': 11, 'verb': 0, 'adjective': 2, 'adverb': 0}\n",
            "{'ORG': 1, 'NORP': 1, 'GPE': 1}\n",
            "{'text': 'outstand movi haunt perform best charact develop ever seen', 'noun': 5, 'verb': 1, 'adjective': 0, 'adverb': 1}\n",
            "{}\n",
            "{'text': 'onli certain peopl relat', 'noun': 3, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'must put smile satisfact heath ledger face right heaven', 'noun': 4, 'verb': 1, 'adjective': 1, 'adverb': 2}\n",
            "{}\n",
            "{'text': 'the hype real', 'noun': 1, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'masterpiec Ã°Â\\x9fÂ\\x98Â\\x8d', 'noun': 2, 'verb': 0, 'adjective': 0, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'just amaz how movi exist', 'noun': 0, 'verb': 1, 'adjective': 1, 'adverb': 1}\n",
            "{}\n",
            "{'text': 'went second time watch', 'noun': 2, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{'ORDINAL': 1}\n",
            "{'text': 'a psycholog studi rather superhero flick', 'noun': 3, 'verb': 0, 'adjective': 0, 'adverb': 1}\n",
            "{}\n",
            "{'text': 'joaquin oscar joker best dark suspens thriller darker dark knight', 'noun': 6, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{'PERSON': 2, 'PRODUCT': 1}\n",
            "{'text': 'venic review', 'noun': 1, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{'DATE': 1}\n",
            "{'text': 'final real movi', 'noun': 1, 'verb': 0, 'adjective': 2, 'adverb': 0}\n",
            "{'ORG': 1}\n",
            "{'text': 'good lord', 'noun': 1, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'oscar phoenix', 'noun': 2, 'verb': 0, 'adjective': 0, 'adverb': 0}\n",
            "{'GPE': 1}\n",
            "{'text': 'not spoon feed cgi fuel faux drama', 'noun': 4, 'verb': 1, 'adjective': 0, 'adverb': 2}\n",
            "{'ORG': 1}\n",
            "{'text': 'critic useless', 'noun': 1, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'joker endgam', 'noun': 2, 'verb': 0, 'adjective': 0, 'adverb': 0}\n",
            "{'PERSON': 2}\n",
            "{'text': 'i would call masterpiec', 'noun': 2, 'verb': 1, 'adjective': 0, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'dont forget smile', 'noun': 3, 'verb': 0, 'adjective': 0, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'one best act perform ive ever seen', 'noun': 2, 'verb': 0, 'adjective': 1, 'adverb': 1}\n",
            "{'CARDINAL': 1}\n",
            "{'text': 'ok film', 'noun': 2, 'verb': 0, 'adjective': 0, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'between', 'noun': 0, 'verb': 0, 'adjective': 0, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'extrem overr', 'noun': 2, 'verb': 0, 'adjective': 0, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'believ the hype', 'noun': 1, 'verb': 0, 'adjective': 0, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'masterpiec', 'noun': 1, 'verb': 0, 'adjective': 0, 'adverb': 0}\n",
            "{}\n",
            "\n",
            "CITESEER\n",
            "\n",
            "{'text': 'a maximum entropi approach natur languag process', 'noun': 3, 'verb': 0, 'adjective': 3, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'natur languag process', 'noun': 2, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{'DATE': 1}\n",
            "{'text': 'linguist natur languag process', 'noun': 3, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'natur languag process', 'noun': 2, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{'DATE': 1}\n",
            "{'text': 'statist natur languag process', 'noun': 2, 'verb': 0, 'adjective': 2, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'natur languag processingrobot', 'noun': 2, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{'DATE': 1}\n",
            "{'text': 'tutori natur languag process', 'noun': 2, 'verb': 0, 'adjective': 2, 'adverb': 0}\n",
            "{}\n",
            "{'text': 'natur languag process', 'noun': 2, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{'DATE': 1}\n",
            "{'text': 'natur languag process', 'noun': 2, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{'DATE': 1}\n",
            "{'text': 'natur languag process lyric', 'noun': 3, 'verb': 0, 'adjective': 1, 'adverb': 0}\n",
            "{}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy",
        "colab_type": "text"
      },
      "source": [
        " A constituency parse breaks text into sub-phrases in such a way that there a terminals which are the words in the sentences and unlabeled edges while dependency parsing is the connection of relationships between words.\n"
      ]
    }
  ]
}