{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Larai15/kubernetes/blob/master/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9",
        "colab_type": "text"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF",
        "colab_type": "text"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k",
        "colab_type": "text"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [2019 Dell labtop](https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1) on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film [Joker](https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv) from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag [\"#wuhancoronovirus\"](https://twitter.com/hashtag/wuhancoronovirus) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests         \n",
        "from bs4 import BeautifulSoup  \n",
        "import csv\n",
        "\n",
        "headers = {\n",
        "        'Accept-Encoding': 'gzip, deflate, sdch',\n",
        "        'Accept-Language': 'en-US,en;q=0.8',\n",
        "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "        'Referer': 'http://www.wikipedia.org/',\n",
        "        'Connection': 'keep-alive',\n",
        "}\n",
        "\n",
        "#AMAZON\n",
        "def scrap_amazon(scrap_link):\n",
        "  #scrap_link = \"https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1\"\n",
        "  req = requests.get(scrap_link)\n",
        "  soup = BeautifulSoup(req.content, 'html.parser')\n",
        "  container = soup.findAll('div', attrs={'class':'a-section review aok-relative'})\n",
        "  data = []\n",
        "  for x in container:\n",
        "      Reviewer = x.find('a', attrs={'class':'review-title'}).text\n",
        "      ReviewerName = x.find('span', attrs={'class':'a-profile-name'}).text\n",
        "      data.append([ReviewerName,Reviewer.strip(\"\\n\")])\n",
        "  with open('amazon.csv', 'w', newline='') as csvfile:\n",
        "      csvwriter = csv.writer(csvfile)\n",
        "      csvwriter.writerow(['User','Review'])\n",
        "      for review in data:\n",
        "          csvwriter.writerow(review)\n",
        "\n",
        "#JOKER\n",
        "\n",
        "def scrape_joker(url):\n",
        "    response = requests.get(url,headers=headers, timeout=10)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    reviews = []\n",
        "    users  = []\n",
        "    count  = 0\n",
        "    for j in soup.findAll(\"a\",attrs={'class':'title'}):\n",
        "            if count >= 100:\n",
        "                break\n",
        "            reviews.append(j.text)\n",
        "            count += 1\n",
        "    count  = 0\n",
        "    for j in soup.findAll(\"span\",attrs={'class':'display-name-link'}):\n",
        "            if count >= 100:\n",
        "                break\n",
        "            users.append(j.text)\n",
        "            count += 1\n",
        "\n",
        "    with open('imdb.csv', 'w') as csv_file:\n",
        "        csvwriter = csv.writer(csv_file,delimiter=',')\n",
        "        csvwriter.writerow([\"USER\",\"REVIEW\"])\n",
        "        for i in range(len(reviews)):\n",
        "            csvwriter.writerow([users[i],reviews[i]])\n",
        "def scrap_citeseer(scrap_link):\n",
        "  req = requests.get(scrap_link)\n",
        "  soup = BeautifulSoup(req.content, 'html.parser')\n",
        "  container = soup.findAll('a', attrs={'class':'remove doc_details'})\n",
        "\n",
        "\n",
        "  with open('citeseer.csv', 'w', newline='') as csvfile:\n",
        "      csvwriter = csv.writer(csvfile)\n",
        "      csvwriter.writerow(['PAPER'])\n",
        "      for x in container[:100]:\n",
        "          csvwriter.writerow([str(x.text).strip()])\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    url = \"https://www.amazon.com/Dell-Inspiron-5000-5570-Laptop/dp/B07N49F51N/ref=sr_1_11?crid=1IJ7UWF2F4GHH&keywords=dell%2Bxps%2B15&qid=1580173569&sprefix=dell%2Caps%2C181&sr=8-11&th=1\"\n",
        "    scrap_amazon(url)\n",
        "    url = \"https://www.imdb.com/title/tt7286456/reviews?ref_=tt_urv\"\n",
        "    scrape_joker(url)\n",
        "    url = \"https://citeseerx.ist.psu.edu/search?q=natural+language+processing\"\n",
        "    scrap_citeseer(url)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z",
        "colab_type": "text"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "colab_type": "code",
        "outputId": "5c561e41-1120-47e7-aa28-4cc651fd522d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import csv\n",
        "import pandas as pd \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "def clean_text(train):\n",
        "    result = {}\n",
        "    split_train = train.split()\n",
        "\n",
        "    #1.2\n",
        "    train = train.translate(str.maketrans('', '', string.punctuation)) #remove punctuation\n",
        "    train = train.translate(str.maketrans('', '', string.digits)) #remove digits\n",
        "\n",
        "    train = \" \".join(x for x in train.split() if x not in stop) #remove stop words\n",
        "\n",
        "    train = train.lower() #lowercase\n",
        "\n",
        "    st = PorterStemmer()\n",
        "    train = \" \".join(st.stem(x) for x in train.split())\n",
        "\n",
        "    train = \" \".join(Word(x).lemmatize() for x in train.split())\n",
        "    return train\n",
        "#Amazon\n",
        "cleaned = []\n",
        "with open(\"amazon.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[1]\n",
        "    train = clean_text(train)\n",
        "    row.append(train)\n",
        "    cleaned.append(row)\n",
        "\n",
        "with open(\"amazon_clean.csv\",\"w\") as f:\n",
        "    csvwriter = csv.writer(f)\n",
        "    for l in cleaned:\n",
        "       csvwriter.writerow(l)\n",
        "  \n",
        "\n",
        "\n",
        "#IMDB\n",
        "cleaned = []\n",
        "with open(\"imdb.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[1]\n",
        "    train = clean_text(train)\n",
        "    row.append(train)\n",
        "    cleaned.append(row)\n",
        "\n",
        "with open(\"imdb_clean.csv\",\"w\") as f:\n",
        "    csvwriter = csv.writer(f)\n",
        "    for l in cleaned:\n",
        "       csvwriter.writerow(l)\n",
        "\n",
        "#CITESEER\n",
        "cleaned = []\n",
        "with open(\"citeseer.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[0]\n",
        "    train = clean_text(train)\n",
        "    row.append(train)\n",
        "    cleaned.append(row)\n",
        "\n",
        "with open(\"citeseer_clean.csv\",\"w\") as f:\n",
        "    csvwriter = csv.writer(f)\n",
        "    for l in cleaned:\n",
        "       csvwriter.writerow(l)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0bf6472b9144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#Amazon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"amazon.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0mcsvreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mcsvreader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CLEANED DATA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'amazon.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV",
        "colab_type": "text"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX",
        "colab_type": "text"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr",
        "colab_type": "code",
        "outputId": "fd094277-140a-4078-b197-4339e0a135ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "import csv\n",
        "from pprint import pprint\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "def train_pos_tag(train):\n",
        "    tokens = nltk.word_tokenize(train)\n",
        "    pos_tag = nltk.pos_tag(tokens)\n",
        "    return pos_tag\n",
        "def train_chunk(sentence):\n",
        "    result = {}\n",
        "    doc = nlp(sentence)\n",
        "    # doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "    for ent in doc.ents: \n",
        "        if ent.label_ in result.keys():\n",
        "            result[ent.label_] += 1\n",
        "        else:\n",
        "            result[ent.label_] = 1\n",
        "    return result\n",
        "     \n",
        "def train_parts(post_tag):\n",
        "    count = {\"NN\":0,\"VB\":0,\"JJ\":0,\"RB\":0}\n",
        "    for tag in pos_tag:\n",
        "        if tag[1] in count.keys():\n",
        "            count[tag[1]] += 1\n",
        "    return {\"text\":train,\"noun\":count[\"NN\"],\"verb\":count[\"VB\"],\"adjective\":count[\"JJ\"],\"adverb\":count[\"RB\"]}\n",
        "\n",
        "#Amazon\n",
        "print(\"\\nAMAZON\\n\")\n",
        "cleaned = []\n",
        "with open(\"amazon_clean.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[2]\n",
        "    # print(train)\n",
        "    pos_tag = train_pos_tag(train)\n",
        "    parts = train_parts(pos_tag)\n",
        "    print(parts)\n",
        "    chunk = train_chunk(row[1])\n",
        "    print(chunk)\n",
        "\n",
        "#IMDB\n",
        "print(\"\\imdb\\n\")\n",
        "cleaned = []\n",
        "with open(\"imdb_clean.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[2]\n",
        "    pos_tag = train_pos_tag(train)\n",
        "    parts = train_parts(pos_tag)\n",
        "    print(parts)\n",
        "    chunk = train_chunk(row[1])\n",
        "    print(chunk)\n",
        "\n",
        "\n",
        "\n",
        "#CITESEER\n",
        "print(\"\\nCITESEER\\n\")\n",
        "cleaned = []\n",
        "with open(\"citeseer_clean.csv\", encoding = \"ISO-8859-1\") as f:\n",
        "  csvreader = list(csv.reader(f))\n",
        "  csvreader[0].append(\"CLEANED DATA\")\n",
        "  cleaned.append(csvreader[0])\n",
        "  for row in csvreader[1:]:\n",
        "    train = row[1]\n",
        "    pos_tag = train_pos_tag(train)\n",
        "    parts = train_parts(pos_tag)\n",
        "    print(parts)\n",
        "    chunk = train_chunk(row[1])\n",
        "    print(chunk)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "\n",
            "AMAZON\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bcf119aa214a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAMAZON\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"amazon_clean.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mcsvreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mcsvreader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CLEANED DATA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'amazon_clean.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy",
        "colab_type": "text"
      },
      "source": [
        " A constituency parse breaks text into sub-phrases in such a way that there a terminals which are the words in the sentences and unlabeled edges while dependency parsing is the connection of relationships between words.\n"
      ]
    }
  ]
}